{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caf(tt,k1,k2,k3,k4,k5,k6,k7,k8,par0,bro0,phy0):\n",
    "    '''Takes timepoints and kinetic parameters, calculates concentration time series of caffeine.'''\n",
    "    k9 = k2+k3+k4+k5\n",
    "    y = k1/(k9-k1)*(np.exp(-k1*tt)-np.exp(-k9*tt))\n",
    "    return y\n",
    "\n",
    "def par(tt,k1,k2,k3,k4,k5,k6,k7,k8,par0,bro0,phy0):\n",
    "    '''Takes timepoints and kinetic parameters, calculates concentration time series of paraxanthine.'''\n",
    "    k9 = k2+k3+k4+k5\n",
    "    y = k1*k2/(k9-k1)*(np.exp(-k9*tt)/(k9-k6)-np.exp(-k1*tt)/(k1-k6)+np.exp(-k6*tt)*(k9-k1)/((k9-k6)*(k1-k6)))+par0*np.exp(-k6*tt)\n",
    "    return y\n",
    "\n",
    "def bro(tt,k1,k2,k3,k4,k5,k6,k7,k8,par0,bro0,phy0):\n",
    "    '''Takes timepoints and kinetic parameters, calculates concentration time series of theobromine.'''\n",
    "    k9 = k2+k3+k4+k5\n",
    "    y = k1*k3/(k9-k1)*(np.exp(-k9*tt)/(k9-k7)-np.exp(-k1*tt)/(k1-k7)+np.exp(-k7*tt)*(k9-k1)/((k9-k7)*(k1-k7)))+bro0*np.exp(-k7*tt)\n",
    "    return y\n",
    "\n",
    "def phy(tt,k1,k2,k3,k4,k5,k6,k7,k8,par0,bro0,phy0):\n",
    "    '''Takes timepoints and kinetic parameters, calculates concentration time series of theophylline.'''\n",
    "    k9 = k2+k3+k4+k5\n",
    "    y = k1*k4/(k9-k1)*(np.exp(-k9*tt)/(k9-k8)-np.exp(-k1*tt)/(k1-k8)+np.exp(-k8*tt)*(k9-k1)/((k9-k8)*(k1-k8)))+phy0*np.exp(-k8*tt)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract MRE and CV for time-independent comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_quality_parameters(run_list):\n",
    "    rounder = 2          # decimal places to round numbers\n",
    "    rep_lim  = 3000      # maximum number of bootstrap replicates taken into account\n",
    "\n",
    "    overview = {}\n",
    "    info = pd.read_csv('run_information.csv',index_col=0)\n",
    "    info = info.transpose()\n",
    "\n",
    "    for run in run_list:\n",
    "        # write general fit information\n",
    "        overview[run+' CV'] = {}\n",
    "        overview[run+' MRE'] = {}\n",
    "        for information in ['mc_replicates','c0','loss','error%']:\n",
    "            overview[run+' MRE'][information.strip(' ')] = info.loc[information,run]\n",
    "            overview[run+' CV'][information.strip(' ')] = info.loc[information,run]\n",
    "\n",
    "        # load fitted data\n",
    "        with open(f'runs_manuscript/{run}.txt','r') as file:\n",
    "            lines = file.readlines()\n",
    "        parameters = []\n",
    "        fits       = []\n",
    "        timepoints = []\n",
    "        sv_values  = []\n",
    "        for line in lines:\n",
    "            if 'ORIGINAL' in line:\n",
    "                for i in line.split():\n",
    "                    if i[0].isnumeric():\n",
    "                        parameters.append(float(i))\n",
    "            elif 'TIMEPOINTS' in line:\n",
    "                for i in line.split():\n",
    "                    if i[0].isnumeric():\n",
    "                        timepoints.append(float(i))\n",
    "            elif 'SV_VALUES' in line:\n",
    "                tmp = []\n",
    "                for i in line.split():\n",
    "                    if i[0].isnumeric():\n",
    "                        tmp.append(float(i))\n",
    "                sv_values.append(tmp)\n",
    "            elif line[0].isnumeric():\n",
    "                tmp = []\n",
    "                for i in line.split():\n",
    "                    tmp.append(float(i))\n",
    "                fits.append(tmp)\n",
    "\n",
    "        fits      = fits[:rep_lim]\n",
    "        sv_values = sv_values[:rep_lim]\n",
    "        # write more general data\n",
    "        overview[run+' CV']['n_samples'] = len(fits)\n",
    "        overview[run+' CV']['n_timepoints'] = len(timepoints)\n",
    "        overview[run+' MRE']['n_samples'] = len(fits)\n",
    "        overview[run+' MRE']['n_timepoints'] = len(timepoints)\n",
    "\n",
    "\n",
    "        # extract kinetic data\n",
    "        timepoints = np.array(timepoints)\n",
    "        fits       = np.array(fits)                \n",
    "        sv_values  = np.array(sv_values)\n",
    "        parameter_names = ['kcincaf','kcafpar','kcafbro','kcafphy','kcafdeg','kpardeg','kbrodeg','kphydeg','par0','bro0','phy0','f1','f2','f3','f4','f5','f6','f7','f8','f9','f10','f11','f12','f13','f14','f15','f16','f17','f18','f19','f20','f21','f22','f23','f24']\n",
    "        label_names     = []\n",
    "        for i in range(len(parameters)):\n",
    "            sigma  = np.sqrt(np.sum((fits[:,i]-parameters[i])**2)/fits.shape[0])\n",
    "            median = np.median((fits[:,i]-parameters[i])/parameters[i])\n",
    "            s      = np.std(fits[:,i])\n",
    "            rel_sigma = sigma/parameters[i]*100\n",
    "            rel_s     = s/parameters[i]*100\n",
    "            overview[run+' CV'][parameter_names[i]] = round(rel_sigma,rounder)\n",
    "            overview[run+' MRE'][parameter_names[i]] = round(median*100,rounder)\n",
    "\n",
    "        # extract sweat volume data\n",
    "        sv_rel_diffs = []\n",
    "        sv_abs_diffs = []\n",
    "        outlier_number = 0\n",
    "        for i in range(fits.shape[0]):\n",
    "            tmp_diffs     = fits[i,11:]-sv_values[i]\n",
    "            tmp_rel_diffs = tmp_diffs/sv_values[i]\n",
    "            for i in tmp_rel_diffs:\n",
    "                if i < 10:\n",
    "                    sv_rel_diffs.append(i)\n",
    "                else:\n",
    "                    outlier_number += 1\n",
    "            sv_abs_diffs.append(tmp_diffs)\n",
    "        print(f'{run}: Removed {outlier_number} outlier(s) of SV statistics. This is {round(outlier_number/len(np.array(sv_values).flatten())*100,5)} % of the whole dataset.')\n",
    "        sv_rel_diffs = np.array(sv_rel_diffs).flatten()\n",
    "        sv_abs_diffs = np.array(sv_abs_diffs).flatten()\n",
    "        rel_sigma = np.sqrt(np.sum(sv_rel_diffs**2)/len(sv_rel_diffs))\n",
    "        rel_s     = np.std(sv_rel_diffs)\n",
    "        abs_sigma = np.sqrt(np.sum(sv_abs_diffs**2)/len(sv_abs_diffs))\n",
    "        median_rel = np.median(sv_rel_diffs)\n",
    "        overview[run+' CV']['sv'] = round(rel_sigma*100,rounder)\n",
    "        overview[run+' MRE']['sv'] = round(median_rel*100,rounder)\n",
    "\n",
    "        # extract concentration data\n",
    "        for func, name in zip([caf,par,bro,phy],['caf','par','bro','phy']):\n",
    "            true_conc     = func(timepoints,*parameters)\n",
    "            fitted_matrix = []\n",
    "            for i in range(fits.shape[0]):\n",
    "                fitted = func(timepoints,*fits[i,:11])\n",
    "                fitted_matrix.append(fitted)\n",
    "            fitted_matrix = np.array(fitted_matrix)\n",
    "\n",
    "            rel_sigma_list = []\n",
    "            rel_s_list     = []\n",
    "            median_list    = []\n",
    "            for i in range(fitted_matrix.shape[1]):\n",
    "                abs_sigma = np.sqrt(np.sum((fitted_matrix[:,i]-true_conc[i])**2)/fitted_matrix.shape[0])\n",
    "                abs_s     = np.std(fitted_matrix[:,i])\n",
    "                median_list.append((fitted_matrix[:,i]-true_conc[i])/true_conc[i])\n",
    "                rel_sigma = abs_sigma/true_conc[i]*100\n",
    "                rel_s     = abs_s/true_conc[i]*100\n",
    "                rel_sigma_list.append(rel_sigma)\n",
    "                rel_s_list.append(rel_s)\n",
    "            overview[run+' CV'][name+' conc.'] = round(np.nanmean(rel_sigma_list[1:]),rounder)\n",
    "            median = np.nanmedian(median_list)\n",
    "            overview[run+' MRE'][name+' conc.'] = round(median*100,rounder)\n",
    "\n",
    "    # add an average row\n",
    "    for run in overview:\n",
    "        tmp = []\n",
    "        for i in overview[run]:\n",
    "            if i not in ['mc_replicates','c0','loss','n_samples','n_timepoints','error%']:\n",
    "                tmp.append(overview[run][i])\n",
    "        overview[run]['total_avg'] = round(np.mean(np.abs(tmp)),rounder)\n",
    "\n",
    "    # put everything in a nice DataFrame\n",
    "    overview_df = pd.DataFrame(overview)\n",
    "    # save as csv\n",
    "    # overview_df.to_csv('analysis_tables/v_sweat_norm.csv')\n",
    "\n",
    "    # make column lists for MRE and CV for easier view\n",
    "    CV  = []\n",
    "    MRE = []\n",
    "    for i in overview_df.columns:\n",
    "        if 'MRE' in i:\n",
    "            MRE.append(i)\n",
    "        else:\n",
    "            CV.append(i)\n",
    "    return MRE, CV, overview_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_104: Removed 1 outlier(s) of SV statistics. This is 0.01667 % of the whole dataset.\n",
      "run_108: Removed 2 outlier(s) of SV statistics. This is 0.03333 % of the whole dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/mgotsmy/envs/robust_loss/lib/python3.7/site-packages/ipykernel_launcher.py:107: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/users/mgotsmy/envs/robust_loss/lib/python3.7/site-packages/ipykernel_launcher.py:108: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/users/mgotsmy/envs/robust_loss/lib/python3.7/site-packages/ipykernel_launcher.py:109: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_109: Removed 0 outlier(s) of SV statistics. This is 0.0 % of the whole dataset.\n",
      "run_110: Removed 3 outlier(s) of SV statistics. This is 0.05 % of the whole dataset.\n",
      "run_111: Removed 3 outlier(s) of SV statistics. This is 0.05 % of the whole dataset.\n",
      "run_112: Removed 5 outlier(s) of SV statistics. This is 0.08333 % of the whole dataset.\n",
      "run_113: Removed 1 outlier(s) of SV statistics. This is 0.01667 % of the whole dataset.\n",
      "run_121: Removed 2 outlier(s) of SV statistics. This is 0.03333 % of the whole dataset.\n"
     ]
    }
   ],
   "source": [
    "### Supplementary Notes: Sensitivity Analysis\n",
    "#   Following Comparisons have been done\n",
    "### \n",
    "\n",
    "    # different loss functions\n",
    "losses    = ['run_104','run_108','run_109','run_110','run_111','run_112','run_113','run_121']\n",
    "    # different MC replicates\n",
    "motecarlo = ['run_138','run_123','run_121','run_124']\n",
    "    # how many timepoints to use for fitting?\n",
    "timepoints = ['run_121','run_125','run_126','run_127','run_128','run_129','run_130']\n",
    "    # different error estimates\n",
    "errorsize = ['run_133','run_121','run_131','run_135']\n",
    "    # difference with without SV normalization\n",
    "v_sweat_norm   = ['run_136','run_134']\n",
    "\n",
    "\n",
    "MRE,CV, overview = extract_quality_parameters(losses) # you can try all different lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_104 CV</th>\n",
       "      <th>run_108 CV</th>\n",
       "      <th>run_109 CV</th>\n",
       "      <th>run_110 CV</th>\n",
       "      <th>run_111 CV</th>\n",
       "      <th>run_112 CV</th>\n",
       "      <th>run_113 CV</th>\n",
       "      <th>run_121 CV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mc_replicates</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>cauchy</td>\n",
       "      <td>huber</td>\n",
       "      <td>soft_l1</td>\n",
       "      <td>robust</td>\n",
       "      <td>max_cauchy</td>\n",
       "      <td>max_huber</td>\n",
       "      <td>max_soft_l1</td>\n",
       "      <td>max_robust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>error%</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_samples</th>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_timepoints</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kcincaf</th>\n",
       "      <td>39.19</td>\n",
       "      <td>35.8</td>\n",
       "      <td>35.15</td>\n",
       "      <td>36.75</td>\n",
       "      <td>29.79</td>\n",
       "      <td>30.01</td>\n",
       "      <td>32.49</td>\n",
       "      <td>25.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kcafpar</th>\n",
       "      <td>25.48</td>\n",
       "      <td>23.48</td>\n",
       "      <td>23.85</td>\n",
       "      <td>16.7</td>\n",
       "      <td>18.2</td>\n",
       "      <td>17.76</td>\n",
       "      <td>16.81</td>\n",
       "      <td>19.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kcafbro</th>\n",
       "      <td>24.52</td>\n",
       "      <td>23.1</td>\n",
       "      <td>22.71</td>\n",
       "      <td>23.93</td>\n",
       "      <td>15.94</td>\n",
       "      <td>16.12</td>\n",
       "      <td>15.61</td>\n",
       "      <td>19.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kcafphy</th>\n",
       "      <td>24.81</td>\n",
       "      <td>22.62</td>\n",
       "      <td>23.13</td>\n",
       "      <td>24.18</td>\n",
       "      <td>16.89</td>\n",
       "      <td>15.76</td>\n",
       "      <td>15.65</td>\n",
       "      <td>18.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kcafdeg</th>\n",
       "      <td>81.55</td>\n",
       "      <td>70.77</td>\n",
       "      <td>78.82</td>\n",
       "      <td>55.19</td>\n",
       "      <td>48.68</td>\n",
       "      <td>78.73</td>\n",
       "      <td>78.03</td>\n",
       "      <td>41.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kpardeg</th>\n",
       "      <td>35.6</td>\n",
       "      <td>33.33</td>\n",
       "      <td>34.78</td>\n",
       "      <td>22.88</td>\n",
       "      <td>24.28</td>\n",
       "      <td>32.79</td>\n",
       "      <td>32.75</td>\n",
       "      <td>22.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kbrodeg</th>\n",
       "      <td>42.88</td>\n",
       "      <td>41.69</td>\n",
       "      <td>41.75</td>\n",
       "      <td>40.96</td>\n",
       "      <td>29.37</td>\n",
       "      <td>39.11</td>\n",
       "      <td>38.58</td>\n",
       "      <td>28.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kphydeg</th>\n",
       "      <td>42.61</td>\n",
       "      <td>41.32</td>\n",
       "      <td>40.9</td>\n",
       "      <td>42.11</td>\n",
       "      <td>29.77</td>\n",
       "      <td>37.78</td>\n",
       "      <td>38.32</td>\n",
       "      <td>26.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>par0</th>\n",
       "      <td>25.99</td>\n",
       "      <td>24.43</td>\n",
       "      <td>24.18</td>\n",
       "      <td>21.63</td>\n",
       "      <td>17.44</td>\n",
       "      <td>18.22</td>\n",
       "      <td>18.26</td>\n",
       "      <td>17.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bro0</th>\n",
       "      <td>25.16</td>\n",
       "      <td>24.96</td>\n",
       "      <td>23.46</td>\n",
       "      <td>35.14</td>\n",
       "      <td>16.68</td>\n",
       "      <td>18.02</td>\n",
       "      <td>17.35</td>\n",
       "      <td>18.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phy0</th>\n",
       "      <td>25.52</td>\n",
       "      <td>24.48</td>\n",
       "      <td>24.11</td>\n",
       "      <td>35.92</td>\n",
       "      <td>17.15</td>\n",
       "      <td>17.16</td>\n",
       "      <td>16.86</td>\n",
       "      <td>18.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sv</th>\n",
       "      <td>45.93</td>\n",
       "      <td>41.79</td>\n",
       "      <td>43.97</td>\n",
       "      <td>33.29</td>\n",
       "      <td>24.09</td>\n",
       "      <td>26.93</td>\n",
       "      <td>23.64</td>\n",
       "      <td>19.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caf conc.</th>\n",
       "      <td>22.55</td>\n",
       "      <td>20.49</td>\n",
       "      <td>21.4</td>\n",
       "      <td>16.08</td>\n",
       "      <td>17.8</td>\n",
       "      <td>28.68</td>\n",
       "      <td>28.5</td>\n",
       "      <td>12.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>par conc.</th>\n",
       "      <td>23.06</td>\n",
       "      <td>21.06</td>\n",
       "      <td>22.05</td>\n",
       "      <td>16.98</td>\n",
       "      <td>18.22</td>\n",
       "      <td>29.58</td>\n",
       "      <td>29.66</td>\n",
       "      <td>14.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bro conc.</th>\n",
       "      <td>22.85</td>\n",
       "      <td>21.18</td>\n",
       "      <td>22.1</td>\n",
       "      <td>19.46</td>\n",
       "      <td>18.31</td>\n",
       "      <td>29.46</td>\n",
       "      <td>28.94</td>\n",
       "      <td>14.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phy conc.</th>\n",
       "      <td>22.75</td>\n",
       "      <td>21.1</td>\n",
       "      <td>22.23</td>\n",
       "      <td>19.77</td>\n",
       "      <td>18.33</td>\n",
       "      <td>29.48</td>\n",
       "      <td>29.12</td>\n",
       "      <td>14.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_avg</th>\n",
       "      <td>33.15</td>\n",
       "      <td>30.72</td>\n",
       "      <td>31.54</td>\n",
       "      <td>28.81</td>\n",
       "      <td>22.56</td>\n",
       "      <td>29.1</td>\n",
       "      <td>28.79</td>\n",
       "      <td>20.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              run_104 CV run_108 CV run_109 CV run_110 CV  run_111 CV  \\\n",
       "mc_replicates        100        100        100        100         100   \n",
       "c0                   yes        yes        yes        yes         yes   \n",
       "loss              cauchy      huber    soft_l1     robust  max_cauchy   \n",
       "error%                10         10         10         10          10   \n",
       "n_samples            300        300        300        300         300   \n",
       "n_timepoints          20         20         20         20          20   \n",
       "kcincaf            39.19       35.8      35.15      36.75       29.79   \n",
       "kcafpar            25.48      23.48      23.85       16.7        18.2   \n",
       "kcafbro            24.52       23.1      22.71      23.93       15.94   \n",
       "kcafphy            24.81      22.62      23.13      24.18       16.89   \n",
       "kcafdeg            81.55      70.77      78.82      55.19       48.68   \n",
       "kpardeg             35.6      33.33      34.78      22.88       24.28   \n",
       "kbrodeg            42.88      41.69      41.75      40.96       29.37   \n",
       "kphydeg            42.61      41.32       40.9      42.11       29.77   \n",
       "par0               25.99      24.43      24.18      21.63       17.44   \n",
       "bro0               25.16      24.96      23.46      35.14       16.68   \n",
       "phy0               25.52      24.48      24.11      35.92       17.15   \n",
       "sv                 45.93      41.79      43.97      33.29       24.09   \n",
       "caf conc.          22.55      20.49       21.4      16.08        17.8   \n",
       "par conc.          23.06      21.06      22.05      16.98       18.22   \n",
       "bro conc.          22.85      21.18       22.1      19.46       18.31   \n",
       "phy conc.          22.75       21.1      22.23      19.77       18.33   \n",
       "total_avg          33.15      30.72      31.54      28.81       22.56   \n",
       "\n",
       "              run_112 CV   run_113 CV  run_121 CV  \n",
       "mc_replicates        100          100         100  \n",
       "c0                   yes          yes         yes  \n",
       "loss           max_huber  max_soft_l1  max_robust  \n",
       "error%                10           10          10  \n",
       "n_samples            300          300         300  \n",
       "n_timepoints          20           20          20  \n",
       "kcincaf            30.01        32.49       25.56  \n",
       "kcafpar            17.76        16.81       19.08  \n",
       "kcafbro            16.12        15.61       19.08  \n",
       "kcafphy            15.76        15.65       18.44  \n",
       "kcafdeg            78.73        78.03       41.22  \n",
       "kpardeg            32.79        32.75       22.63  \n",
       "kbrodeg            39.11        38.58       28.69  \n",
       "kphydeg            37.78        38.32       26.86  \n",
       "par0               18.22        18.26       17.64  \n",
       "bro0               18.02        17.35       18.17  \n",
       "phy0               17.16        16.86       18.11  \n",
       "sv                 26.93        23.64       19.66  \n",
       "caf conc.          28.68         28.5       12.88  \n",
       "par conc.          29.58        29.66       14.49  \n",
       "bro conc.          29.46        28.94       14.48  \n",
       "phy conc.          29.48        29.12       14.61  \n",
       "total_avg           29.1        28.79       20.72  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview\n",
    "overview[MRE]\n",
    "overview[CV]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract time-dependent MRE and CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_quality_parameters_time(run_list):\n",
    "    overview = {}\n",
    "    info = pd.read_csv('run_information.csv',index_col=0)\n",
    "    info = info.transpose()\n",
    "    rounder = 2\n",
    "    rep_lim  = 3000\n",
    "\n",
    "    for run in run_list:\n",
    "        overview[run] = {}\n",
    "        overview[run+' median'] = {}\n",
    "\n",
    "\n",
    "        for information in ['mc_replicates','c0','loss','error%']:\n",
    "            overview[run+' median'][information.strip(' ')] = info.loc[information,run]\n",
    "            overview[run][information.strip(' ')] = info.loc[information,run]\n",
    "\n",
    "        # LOAD DATA\n",
    "        with open(f'runs_manuscript/{run}.txt','r') as file:\n",
    "            lines = file.readlines()\n",
    "        parameters = []\n",
    "        fits       = []\n",
    "        timepoints = []\n",
    "        sv_values  = []\n",
    "        for line in lines:\n",
    "            if 'ORIGINAL' in line:\n",
    "                for i in line.split():\n",
    "                    if i[0].isnumeric():\n",
    "                        parameters.append(float(i))\n",
    "            elif 'TIMEPOINTS' in line:\n",
    "                for i in line.split():\n",
    "                    if i[0].isnumeric():\n",
    "                        timepoints.append(float(i))\n",
    "            elif 'SV_VALUES' in line:\n",
    "                tmp = []\n",
    "                for i in line.split():\n",
    "                    if i[0].isnumeric():\n",
    "                        tmp.append(float(i))\n",
    "                sv_values.append(tmp)\n",
    "            elif line[0].isnumeric():\n",
    "                tmp = []\n",
    "                for i in line.split():\n",
    "                    tmp.append(float(i))\n",
    "                fits.append(tmp)\n",
    "\n",
    "        fits      = fits[:rep_lim]\n",
    "        sv_values = sv_values[:rep_lim]\n",
    "        # write general data in dic\n",
    "        overview[run]['n_samples'] = len(fits)\n",
    "        overview[run]['n_timepoints'] = len(timepoints)\n",
    "        overview[run+' median']['n_samples'] = len(fits)\n",
    "        overview[run+' median']['n_timepoints'] = len(timepoints)\n",
    "\n",
    "\n",
    "        # KINETIC DATA\n",
    "        timepoints = np.array(timepoints)\n",
    "        fits       = np.array(fits)                \n",
    "        sv_values  = np.array(sv_values)\n",
    "        parameter_names = ['kcincaf','kcafpar','kcafbro','kcafphy','kcafdeg','kpardeg','kbrodeg','kphydeg','par0','bro0','phy0','f1','f2','f3','f4','f5','f6','f7','f8','f9','f10','f11','f12','f13','f14','f15','f16','f17','f18','f19','f20','f21','f22','f23','f24']\n",
    "        label_names     = []\n",
    "        for i in range(len(parameters)):\n",
    "            sigma  = np.sqrt(np.sum((fits[:,i]-parameters[i])**2)/fits.shape[0])\n",
    "            median = np.median((fits[:,i]-parameters[i])/parameters[i])\n",
    "            rel_sigma = sigma/parameters[i]*100\n",
    "            overview[run][parameter_names[i]] = round(rel_sigma,rounder)\n",
    "            overview[run+' median'][parameter_names[i]] = round(median*100,rounder)\n",
    "\n",
    "        # SWEAT VOLUME DATA\n",
    "        sv_rel_diffs = []\n",
    "        sv_abs_diffs = []\n",
    "        outlier_number = 0\n",
    "        for i in range(fits.shape[0]):\n",
    "            tmp_diffs     = fits[i,11:]-sv_values[i]\n",
    "            tmp_rel_diffs = tmp_diffs/sv_values[i]\n",
    "            for i in tmp_rel_diffs:\n",
    "                if i < 10:\n",
    "                    sv_rel_diffs.append(i)\n",
    "                else:\n",
    "                    outlier_number += 1\n",
    "            sv_abs_diffs.append(tmp_diffs)\n",
    "        print(f'Removed {outlier_number} outlier(s) of SV statistics. This is {round(outlier_number/len(np.array(sv_values).flatten())*100,5)} % of the whole dataset.')\n",
    "        sv_rel_diffs = np.array(sv_rel_diffs).flatten()\n",
    "        sv_abs_diffs = np.array(sv_abs_diffs).flatten()\n",
    "        rel_sigma = np.sqrt(np.sum(sv_rel_diffs**2)/len(sv_rel_diffs))\n",
    "        abs_sigma = np.sqrt(np.sum(sv_abs_diffs**2)/len(sv_abs_diffs))\n",
    "        median_rel = np.median(sv_rel_diffs)\n",
    "        overview[run]['sv'] = round(rel_sigma*100,rounder)\n",
    "        overview[run+' median']['sv'] = round(median_rel*100,rounder)\n",
    "\n",
    "        # time-dependent SWEAT VOLUME DATA\n",
    "        outlier_number = 0\n",
    "        for t in range(timepoints.shape[0]):\n",
    "            sv_rel_diffs = []\n",
    "            sv_abs_diffs = []\n",
    "\n",
    "            for i in range(fits.shape[0]):\n",
    "                tmp_diffs     = fits[i,11+t]-sv_values[i,t]\n",
    "                tmp_rel_diffs = tmp_diffs/sv_values[i,t]\n",
    "                if tmp_rel_diffs < 10:\n",
    "                    sv_rel_diffs.append(tmp_rel_diffs)\n",
    "                else:\n",
    "                    outlier_number += 1\n",
    "                sv_abs_diffs.append(tmp_diffs)\n",
    "\n",
    "            sv_rel_diffs = np.array(sv_rel_diffs).flatten()\n",
    "            sv_abs_diffs = np.array(sv_abs_diffs).flatten()\n",
    "            rel_sigma = np.sqrt(np.sum(sv_rel_diffs**2)/len(sv_rel_diffs))\n",
    "            abs_sigma = np.sqrt(np.sum(sv_abs_diffs**2)/len(sv_abs_diffs))\n",
    "            median_rel = np.median(sv_rel_diffs)\n",
    "            overview[run]['sv t='+str(timepoints[t])] = round(rel_sigma*100,rounder)\n",
    "            overview[run+' median']['sv t='+str(timepoints[t])] = round(median_rel*100,rounder)\n",
    "        #print(f'Removed {outlier_number} outlier(s) of SV statistics. This is {round(outlier_number/len(np.array(sv_abs_diffs).flatten())*100,5)} % of the whole dataset.')\n",
    "        print(f'Removed {outlier_number} outlier(s) of SV statistics. This is {round(outlier_number/len(np.array(sv_values).flatten())*100,5)} % of the whole dataset.')\n",
    "\n",
    "        # CONCENTRATION DATA\n",
    "        for func, name in zip([caf,par,bro,phy],['caf','par','bro','phy']):\n",
    "            true_conc     = func(timepoints,*parameters)\n",
    "            fitted_matrix = []\n",
    "            for i in range(fits.shape[0]):\n",
    "                fitted = func(timepoints,*fits[i,:11])\n",
    "                fitted_matrix.append(fitted)\n",
    "            fitted_matrix = np.array(fitted_matrix)\n",
    "\n",
    "            rel_sigma_list = []\n",
    "            median_list    = []\n",
    "            for i in range(fitted_matrix.shape[1]):\n",
    "                abs_sigma = np.sqrt(np.sum((fitted_matrix[:,i]-true_conc[i])**2)/fitted_matrix.shape[0])\n",
    "                rel_err = (fitted_matrix[:,i]-true_conc[i])/true_conc[i]\n",
    "                median_list.append(rel_err)\n",
    "                rel_sigma = abs_sigma/true_conc[i]*100\n",
    "                rel_sigma_list.append(rel_sigma)\n",
    "                overview[run][name+' conc. t='+str(timepoints[i])] = round(np.nanmean(rel_sigma),rounder)\n",
    "                overview[run+' median'][name+' conc. t='+str(timepoints[i])] = round(np.nanmedian(rel_err)*100,rounder)\n",
    "            if overview[run]['c0'] == '          yes':\n",
    "                overview[run][name+' conc.'] = round(np.nanmean(rel_sigma_list),rounder)\n",
    "            else:\n",
    "                overview[run][name+' conc.'] = round(np.nanmean(rel_sigma_list[1:]),rounder)\n",
    "            median = np.nanmedian(median_list)\n",
    "            overview[run+' median'][name+' conc.'] = round(median*100,rounder)\n",
    "\n",
    "    overview_df = pd.DataFrame(overview)\n",
    "    #overview_df.to_csv('analysis_tables/later_measurements.csv')\n",
    "    return overview_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 7 outlier(s) of SV statistics. This is 0.01167 % of the whole dataset.\n",
      "Removed 7 outlier(s) of SV statistics. This is 0.01167 % of the whole dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/mgotsmy/envs/robust_loss/lib/python3.7/site-packages/ipykernel_launcher.py:127: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/users/mgotsmy/envs/robust_loss/lib/python3.7/site-packages/ipykernel_launcher.py:129: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/users/mgotsmy/envs/robust_loss/lib/python3.7/site-packages/ipykernel_launcher.py:131: RuntimeWarning: Mean of empty slice\n",
      "/home/users/mgotsmy/envs/robust_loss/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1114: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input=overwrite_input)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_134</th>\n",
       "      <th>run_134 median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mc_replicates</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c0</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>max_robust</td>\n",
       "      <td>max_robust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>error%</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_samples</th>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phy conc. t=12.0</th>\n",
       "      <td>16.75</td>\n",
       "      <td>3.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phy conc. t=13.0</th>\n",
       "      <td>18.1</td>\n",
       "      <td>3.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phy conc. t=14.0</th>\n",
       "      <td>19.49</td>\n",
       "      <td>3.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phy conc. t=24.0</th>\n",
       "      <td>35.68</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phy conc.</th>\n",
       "      <td>14.15</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     run_134 run_134 median\n",
       "mc_replicates            100            100\n",
       "c0                       yes            yes\n",
       "loss              max_robust     max_robust\n",
       "error%                    10             10\n",
       "n_samples               3000           3000\n",
       "...                      ...            ...\n",
       "phy conc. t=12.0       16.75           3.15\n",
       "phy conc. t=13.0        18.1           3.05\n",
       "phy conc. t=14.0       19.49           3.04\n",
       "phy conc. t=24.0       35.68            4.2\n",
       "phy conc.              14.15            2.4\n",
       "\n",
       "[122 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# earlier measurments with 15\\dagger timepoints\n",
    "later_measurements = ['run_134']\n",
    "# later maeasurements with 20 timepoints\n",
    "earlier_measurements   = ['run_139']\n",
    "\n",
    "extract_quality_parameters_time(later_measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Robust Loss (3.7)",
   "language": "python",
   "name": "robust_loss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
